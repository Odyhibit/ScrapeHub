# ScrapeHub
Timestamp based scraper for GitHub. Does not keep anything up to date, just downloads all new files, extracts the archives, and renames common file extensions.

These are a couple of scripts that will download a metric ton of files, and then extract them. It takes a while. If there are errors in the extraction, you can just that part again.
The scripts delete the archives upon successful extraction. If there are errors, you might need to examine those files.